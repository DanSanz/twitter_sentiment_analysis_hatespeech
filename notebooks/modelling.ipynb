{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score, f1_score, recall_score\n",
    "from sklearn.metrics import  roc_curve, confusion_matrix, precision_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# function to make n-grams\n",
    "from nltk.util import ngrams \n",
    "from nltk import everygrams\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf = TfidfVectorizer(vocabulary = myvocabulary, stop_words = 'english')\n",
    "# tfs = tfidf.fit_transform(corpus.values())\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import models\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()\n",
    "def tokenize_tweet(s):\n",
    "    \"\"\"\n",
    "    Tokenize each text into a list of words \n",
    "    \"\"\"\n",
    "    tokens = tk(s)\n",
    "    return [w for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/data_for_modelling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the tf-idf object\n",
    "tfidf_vectors = TfidfVectorizer(max_df=0.90, min_df=2, max_features=9000, \n",
    "                                stop_words='english',\n",
    "                                ngram_range=(1, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_only_fit = tfidf_vectors.fit(df['tweet_without_stopwords_and_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling only the fit\n",
    "\n",
    "pickle_out_only_fit = open('tfidf_pickle_fit', 'wb')\n",
    "pickle.dump(tfidf_only_fit, pickle_out_only_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectors.fit_transform(df['tweet_without_stopwords_and_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling the matrix \n",
    "\n",
    "pickle_out_matrix = open('tfidf_pickle', 'wb')\n",
    "pickle.dump(tfidf, pickle_out_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector = pd.DataFrame(tfidf.todense(),columns = tfidf_vectors.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into Training/Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['neg_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train , y_test = train_test_split(df_vector, target, \n",
    "                                                     test_size =.2, random_state=101 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of my training set is (33593, 9000)\n",
      "The shape of my training target is (33593,)\n",
      "The shape of my test set is (8399, 9000)\n",
      "The shape of my test target is (8399,)\n"
     ]
    }
   ],
   "source": [
    "print(f'The shape of my training set is {x_train.shape}')\n",
    "print(f'The shape of my training target is {y_train.shape}')\n",
    "print(f'The shape of my test set is {x_test.shape}')\n",
    "print(f'The shape of my test target is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_val, Y_train , y_val = train_test_split(x_train,y_train, \n",
    "                                                     test_size =.2, random_state=101 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of my final training set is (26874, 9000)\n",
      "The shape of my final training target is (26874,)\n",
      "The shape of my validation set is (6719, 9000)\n",
      "The shape of my validation target is (6719,)\n"
     ]
    }
   ],
   "source": [
    "print(f'The shape of my final training set is {X_train.shape}')\n",
    "print(f'The shape of my final training target is {Y_train.shape}')\n",
    "print(f'The shape of my validation set is {x_val.shape}')\n",
    "print(f'The shape of my validation target is {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on validation\n",
    "y_hat_log = logmodel.predict(x_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9804890096320079"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_hat_log) # calculating f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4271,   36],\n",
       "       [ 122, 3970]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "confusion_matrix(y_test, y_hat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4271, 36, 122, 3970)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_hat_log).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_proba = logmodel.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93023185, 0.06976815],\n",
       "       [0.86738193, 0.13261807],\n",
       "       [0.01654884, 0.98345116],\n",
       "       [0.9510001 , 0.0489999 ],\n",
       "       [0.06449726, 0.93550274],\n",
       "       [0.86595032, 0.13404968],\n",
       "       [0.10116755, 0.89883245],\n",
       "       [0.12675282, 0.87324718],\n",
       "       [0.96427155, 0.03572845],\n",
       "       [0.96041194, 0.03958806]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21489    0\n",
       "14714    0\n",
       "12192    1\n",
       "31163    0\n",
       "6053     1\n",
       "        ..\n",
       "28287    1\n",
       "21944    0\n",
       "26081    0\n",
       "32660    1\n",
       "2474     1\n",
       "Name: neg_label, Length: 8399, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_proba = y_hat_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06976815, 0.13261807, 0.98345116, 0.0489999 , 0.93550274,\n",
       "       0.13404968, 0.89883245, 0.87324718, 0.03572845, 0.03958806])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_neg = Y_hat_proba > .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False,  True,  True])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling the final winning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('pickled_logistic_1', 'wb')\n",
    "pickle.dump(logmodel, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_low</th>\n",
       "      <th>handle_count</th>\n",
       "      <th>no_handle_no_special_no_sin_ash</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>character_count</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_1</th>\n",
       "      <th>lemmatized_final</th>\n",
       "      <th>tweet_without_stopwords</th>\n",
       "      <th>tweet_without_stopwords_and_2</th>\n",
       "      <th>neg_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13 days to go #gettingthere</td>\n",
       "      <td>0</td>\n",
       "      <td>days to go #gettingthere</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>day to go # gettingthere</td>\n",
       "      <td>day to go #gettingthere</td>\n",
       "      <td>day to go # gettingthere</td>\n",
       "      <td>day go # gettingthere</td>\n",
       "      <td>day gettingthere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@anggxo get off my twitter fag</td>\n",
       "      <td>1</td>\n",
       "      <td>get off my twitter fag</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>get off my twitter fag</td>\n",
       "      <td>get off my twitter fag</td>\n",
       "      <td>get off my twitter fag</td>\n",
       "      <td>get twitter fag</td>\n",
       "      <td>get twitter fag</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>these hoes got more bodies than a cemetery&amp;#12...</td>\n",
       "      <td>0</td>\n",
       "      <td>these hoes got more bodies than a cemetery    ...</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>these hoe get more body than a cemetery</td>\n",
       "      <td>these hoe get more body than a cemetery</td>\n",
       "      <td>these hoe get more body than a cemetery</td>\n",
       "      <td>hoe get body cemetery</td>\n",
       "      <td>hoe get body cemetery</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>a friend just told me she's afraid to go to dc...</td>\n",
       "      <td>1</td>\n",
       "      <td>a friend just told me she's afraid to go to dc...</td>\n",
       "      <td>124</td>\n",
       "      <td>95</td>\n",
       "      <td>a friend just tell me she 's afraid to go to d...</td>\n",
       "      <td>a friend just tell me she 's afraid to go to d...</td>\n",
       "      <td>a friend just tell me she's afraid to go to dc...</td>\n",
       "      <td>friend tell 's afraid go dc rally amp attack #...</td>\n",
       "      <td>friend tell afraid rally amp attack berniebros...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>i've noticed a lot of #icontf16 presentations ...</td>\n",
       "      <td>1</td>\n",
       "      <td>i've noticed a lot of #icontf   presentations ...</td>\n",
       "      <td>117</td>\n",
       "      <td>95</td>\n",
       "      <td>i 've notice a lot of # icontf presentation me...</td>\n",
       "      <td>i 've notice a lot of #icontf presentation men...</td>\n",
       "      <td>i've notice a lot of # icontf presentation men...</td>\n",
       "      <td>'ve notice lot # icontf presentation mention h...</td>\n",
       "      <td>'ve notice lot icontf presentation mention hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41987</td>\n",
       "      <td>@_elenaraquel_ its swag bitch aha</td>\n",
       "      <td>1</td>\n",
       "      <td>its swag bitch aha</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>its swag bitch aha</td>\n",
       "      <td>its swag bitch aha</td>\n",
       "      <td>its swag bitch aha</td>\n",
       "      <td>swag bitch aha</td>\n",
       "      <td>swag bitch aha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41988</td>\n",
       "      <td>when quay collab with @user says sold out!!!ð...</td>\n",
       "      <td>1</td>\n",
       "      <td>when quay collab with  says sold out          ...</td>\n",
       "      <td>121</td>\n",
       "      <td>54</td>\n",
       "      <td>when quay collab with say sell out # noooo # w...</td>\n",
       "      <td>when quay collab with say sell out #noooo #why...</td>\n",
       "      <td>when quay collab with say sell out # noooo # w...</td>\n",
       "      <td>quay collab say sell # noooo # whyyyy # loveyo...</td>\n",
       "      <td>quay collab say sell noooo whyyyy loveyoudesi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41989</td>\n",
       "      <td>rt @_groovymovie: &amp;#8220;@shane_a1: hmu talmbo...</td>\n",
       "      <td>2</td>\n",
       "      <td>rt         hmu talmbout match but when i pull ...</td>\n",
       "      <td>114</td>\n",
       "      <td>73</td>\n",
       "      <td>rt hmu talmbout match but when i pull up its o...</td>\n",
       "      <td>rt hmu talmbout match but when i pull up its o...</td>\n",
       "      <td>rt hmu talmbout match but when i pull up its o...</td>\n",
       "      <td>rt hmu talmbout match pull niccas wit ya smfh ...</td>\n",
       "      <td>hmu talmbout match pull niccas wit smfh shit l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41990</td>\n",
       "      <td>rt @obey_jrock__: this is a true ride or die b...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt   this is a true ride or die bitch</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>rt this be a true ride or die bitch</td>\n",
       "      <td>rt this be a true ride or die bitch</td>\n",
       "      <td>rt this be a true ride or die bitch</td>\n",
       "      <td>rt true ride die bitch</td>\n",
       "      <td>true ride die bitch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41991</td>\n",
       "      <td>rt @allhailtaron_: i got the deals for the low...</td>\n",
       "      <td>1</td>\n",
       "      <td>rt   i got the deals for the low  i know you h...</td>\n",
       "      <td>114</td>\n",
       "      <td>77</td>\n",
       "      <td>rt i get the deal for the low i know you hoe l...</td>\n",
       "      <td>rt i get the deal for the low i know you hoe l...</td>\n",
       "      <td>rt i get the deal for the low i know you hoe l...</td>\n",
       "      <td>rt get deal low know hoe lonely fuck cuff seas...</td>\n",
       "      <td>get deal low know hoe lonely fuck cuff season ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41992 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet_low  handle_count  \\\n",
       "0                         13 days to go #gettingthere                0   \n",
       "1                         @anggxo get off my twitter fag             1   \n",
       "2      these hoes got more bodies than a cemetery&#12...             0   \n",
       "3      a friend just told me she's afraid to go to dc...             1   \n",
       "4      i've noticed a lot of #icontf16 presentations ...             1   \n",
       "...                                                  ...           ...   \n",
       "41987                  @_elenaraquel_ its swag bitch aha             1   \n",
       "41988  when quay collab with @user says sold out!!!ð...             1   \n",
       "41989  rt @_groovymovie: &#8220;@shane_a1: hmu talmbo...             2   \n",
       "41990  rt @obey_jrock__: this is a true ride or die b...             1   \n",
       "41991  rt @allhailtaron_: i got the deals for the low...             1   \n",
       "\n",
       "                         no_handle_no_special_no_sin_ash  tweet_length  \\\n",
       "0                            days to go #gettingthere               30   \n",
       "1                                 get off my twitter fag            23   \n",
       "2      these hoes got more bodies than a cemetery    ...            58   \n",
       "3      a friend just told me she's afraid to go to dc...           124   \n",
       "4      i've noticed a lot of #icontf   presentations ...           117   \n",
       "...                                                  ...           ...   \n",
       "41987                                 its swag bitch aha            19   \n",
       "41988  when quay collab with  says sold out          ...           121   \n",
       "41989  rt         hmu talmbout match but when i pull ...           114   \n",
       "41990      rt   this is a true ride or die bitch                    45   \n",
       "41991  rt   i got the deals for the low  i know you h...           114   \n",
       "\n",
       "       character_count                                         lemmatized  \\\n",
       "0                   21                           day to go # gettingthere   \n",
       "1                   18                             get off my twitter fag   \n",
       "2                   35            these hoe get more body than a cemetery   \n",
       "3                   95  a friend just tell me she 's afraid to go to d...   \n",
       "4                   95  i 've notice a lot of # icontf presentation me...   \n",
       "...                ...                                                ...   \n",
       "41987               15                                 its swag bitch aha   \n",
       "41988               54  when quay collab with say sell out # noooo # w...   \n",
       "41989               73  rt hmu talmbout match but when i pull up its o...   \n",
       "41990               27                rt this be a true ride or die bitch   \n",
       "41991               77  rt i get the deal for the low i know you hoe l...   \n",
       "\n",
       "                                            lemmatized_1  \\\n",
       "0                                day to go #gettingthere   \n",
       "1                                 get off my twitter fag   \n",
       "2                these hoe get more body than a cemetery   \n",
       "3      a friend just tell me she 's afraid to go to d...   \n",
       "4      i 've notice a lot of #icontf presentation men...   \n",
       "...                                                  ...   \n",
       "41987                                 its swag bitch aha   \n",
       "41988  when quay collab with say sell out #noooo #why...   \n",
       "41989  rt hmu talmbout match but when i pull up its o...   \n",
       "41990                rt this be a true ride or die bitch   \n",
       "41991  rt i get the deal for the low i know you hoe l...   \n",
       "\n",
       "                                        lemmatized_final  \\\n",
       "0                               day to go # gettingthere   \n",
       "1                                 get off my twitter fag   \n",
       "2                these hoe get more body than a cemetery   \n",
       "3      a friend just tell me she's afraid to go to dc...   \n",
       "4      i've notice a lot of # icontf presentation men...   \n",
       "...                                                  ...   \n",
       "41987                                 its swag bitch aha   \n",
       "41988  when quay collab with say sell out # noooo # w...   \n",
       "41989  rt hmu talmbout match but when i pull up its o...   \n",
       "41990                rt this be a true ride or die bitch   \n",
       "41991  rt i get the deal for the low i know you hoe l...   \n",
       "\n",
       "                                 tweet_without_stopwords  \\\n",
       "0                                  day go # gettingthere   \n",
       "1                                        get twitter fag   \n",
       "2                                  hoe get body cemetery   \n",
       "3      friend tell 's afraid go dc rally amp attack #...   \n",
       "4      've notice lot # icontf presentation mention h...   \n",
       "...                                                  ...   \n",
       "41987                                     swag bitch aha   \n",
       "41988  quay collab say sell # noooo # whyyyy # loveyo...   \n",
       "41989  rt hmu talmbout match pull niccas wit ya smfh ...   \n",
       "41990                             rt true ride die bitch   \n",
       "41991  rt get deal low know hoe lonely fuck cuff seas...   \n",
       "\n",
       "                           tweet_without_stopwords_and_2  neg_label  \n",
       "0                                       day gettingthere          0  \n",
       "1                                        get twitter fag          1  \n",
       "2                                  hoe get body cemetery          1  \n",
       "3      friend tell afraid rally amp attack berniebros...          0  \n",
       "4      've notice lot icontf presentation mention hap...          0  \n",
       "...                                                  ...        ...  \n",
       "41987                                     swag bitch aha          1  \n",
       "41988      quay collab say sell noooo whyyyy loveyoudesi          0  \n",
       "41989  hmu talmbout match pull niccas wit smfh shit l...          1  \n",
       "41990                                true ride die bitch          1  \n",
       "41991  get deal low know hoe lonely fuck cuff season ...          1  \n",
       "\n",
       "[41992 rows x 11 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will be used throughout the modeling process\n",
    "\n",
    "epochs = 10  \n",
    "batch_size = 64  \n",
    "sequence_len = 23  # Maximum number of words in a sequence\n",
    "embedding_dimensions = 100  # Number of dimensions of the trainable embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tknz = TweetTokenizer()\n",
    "\n",
    "def tokenize_tweet(s):\n",
    "    \"\"\"\n",
    "    Tokenize each text into a list of words removing the ashtags\n",
    "    \"\"\"\n",
    "    tokens = tknz.tokenize(s)\n",
    "    return [w for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tweet_without_stopwords_and_2'].apply(tokenize_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      [day, gettingthere]\n",
       "1                                      [get, twitter, fag]\n",
       "2                               [hoe, get, body, cemetery]\n",
       "3        [friend, tell, afraid, rally, amp, attack, ber...\n",
       "4        [', ve, notice, lot, icontf, presentation, men...\n",
       "                               ...                        \n",
       "41987                                   [swag, bitch, aha]\n",
       "41988    [quay, collab, say, sell, noooo, whyyyy, lovey...\n",
       "41989    [hmu, talmbout, match, pull, niccas, wit, smfh...\n",
       "41990                             [true, ride, die, bitch]\n",
       "41991    [get, deal, low, know, hoe, lonely, fuck, cuff...\n",
       "Name: tokenized, Length: 41992, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tokenized'], df['neg_label'], test_size=0.2, random_state=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=20000, # The 20000 most important words for our vocabulary\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n', # Regex that will prune off non alphabetical characters\n",
    "               lower=True, # Lowercase the text data for consistency\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train) # Fit on our training set\n",
    "X_train_seq = tk.texts_to_sequences(X_train) # Indexed sequences based on fitted tokenizer\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def max_seq_length (sequence):\n",
    "    length = []\n",
    "    \n",
    "    for i in range(0, len(sequence)):\n",
    "        length.append(len(sequence[i]))\n",
    "    return max(length)\n",
    "\n",
    "    \n",
    "   \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of max_sequence is 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length(X_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # 19000 seems to be range of vocabulary\n",
    "# for i in range(0, len(X_train_seq)-1):\n",
    "#     highest_words = []\n",
    "#     highest_words.append(max(X_train_seq[i]))\n",
    "#     print(highest_words)\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_pad = pad_sequences(X_train_seq, maxlen=max_length) # Padding the sequences so that they are all the same length\n",
    "X_test_seq_pad = pad_sequences(X_test_seq, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    2,  460,    4,   17,    3,  122,  124, 1390,   16,\n",
       "          1], dtype=int32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for one length\n",
    "X_train_seq_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_pad, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will be used throughout the modeling process\n",
    "\n",
    "epochs = 10  \n",
    "batch_size = 64  \n",
    "embedding_dimensions = 100  # Number of dimensions of the trainable embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26874 samples, validate on 6719 samples\n",
      "Epoch 1/10\n",
      "26874/26874 [==============================] - 17s 623us/step - loss: 0.1723 - accuracy: 0.9324 - val_loss: 0.0578 - val_accuracy: 0.9836\n",
      "Epoch 2/10\n",
      "26874/26874 [==============================] - 17s 620us/step - loss: 0.0326 - accuracy: 0.9915 - val_loss: 0.0571 - val_accuracy: 0.9839\n",
      "Epoch 3/10\n",
      "26874/26874 [==============================] - 16s 610us/step - loss: 0.0178 - accuracy: 0.9961 - val_loss: 0.0662 - val_accuracy: 0.9829\n",
      "Epoch 4/10\n",
      "26874/26874 [==============================] - 16s 594us/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.0743 - val_accuracy: 0.9814\n",
      "Epoch 5/10\n",
      "26874/26874 [==============================] - 16s 601us/step - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0841 - val_accuracy: 0.9807\n",
      "Epoch 6/10\n",
      "26874/26874 [==============================] - 17s 618us/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.0988 - val_accuracy: 0.9805\n",
      "Epoch 7/10\n",
      "26874/26874 [==============================] - 16s 608us/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0969 - val_accuracy: 0.9789\n",
      "Epoch 8/10\n",
      "26874/26874 [==============================] - 17s 638us/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.1076 - val_accuracy: 0.9780\n",
      "Epoch 9/10\n",
      "26874/26874 [==============================] - 18s 652us/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.1143 - val_accuracy: 0.9766\n",
      "Epoch 10/10\n",
      "26874/26874 [==============================] - 18s 659us/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.1230 - val_accuracy: 0.9774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emb_model = models.Sequential()\n",
    "emb_model.add(layers.Embedding(number_of_words, embedding_dimensions, input_length=max_length))\n",
    "emb_model.add(layers.Conv1D(32,\n",
    "                            5,\n",
    "                            activation='relu',\n",
    "                            input_shape=(100,1)))\n",
    "emb_model.add(layers.GlobalMaxPooling1D())\n",
    "emb_model.add(layers.Dropout(0.2))\n",
    "emb_model.add(layers.Dense(64,activation='relu',))\n",
    "emb_model.add(layers.Dropout(0.2))\n",
    "emb_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "emb_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "emb_history = emb_model.fit(X_train_emb,\n",
    "                            y_train_emb,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_valid_emb, y_valid_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8399/8399 [==============================] - 0s 59us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1076549892671105, 0.9792832732200623]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.evaluate(X_test_seq_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_for_test = emb_model.predict_proba(X_test_seq_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.6107068e-03],\n",
       "       [1.0000000e+00],\n",
       "       [8.2513674e-10],\n",
       "       [2.0167288e-06],\n",
       "       [9.9986053e-01],\n",
       "       [5.8676579e-07],\n",
       "       [9.9996185e-01],\n",
       "       [1.0378705e-02],\n",
       "       [9.9929070e-01],\n",
       "       [9.9817097e-01]], dtype=float32)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_for_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_for_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_cnn = open('pickle_cnn', 'wb')\n",
    "pickle.dump(emb_model,pickle_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=9000)\n",
    "\n",
    "X = cv.fit_transform(df['tweet_without_stopwords_and_2']).toarray()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size = 0.2, random_state = 10)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out_cv = open('CVBayes', 'wb')\n",
    "pickle.dump(cv, pickle_out_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the Naives Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out_bayes = open('BayesModel', 'wb')\n",
    "pickle.dump (classifier, pickle_out_bayes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2402, 1852],\n",
       "       [ 195, 3950]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
