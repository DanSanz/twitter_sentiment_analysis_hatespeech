{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the necessary libraries for the custom_functions\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# for the lemmatizer function\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# for the word frequency function\n",
    "from gensim.corpora import Dictionary\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# for the wordcloud function\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for the Tweet Tokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# for the roc plot\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    \n",
    "    \"\"\" Function replacing a specific regex pattern with an empty space\"\"\"\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting(input_txt, pattern):\n",
    "    \n",
    "    \"\"\"Simple function returning the pattern count instances in each tweet\"\"\"\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    return len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    \n",
    "    \"\"\" Function defining the actual part of speech as adjective, \n",
    "    verb, noun or adverb\"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    \n",
    "    \"\"\"Function to lemmatize with POS all tweets\"\"\"\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "            # 'ass' kept being reduced to 'as' for some reason         \n",
    "        if word == 'ass':\n",
    "            lemmatized_sentence.append(word)\n",
    "        \n",
    "        elif tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknz = TweetTokenizer()\n",
    "\n",
    "def tokenize_tweet(s):\n",
    "    \"\"\"\n",
    "    Tokenize each tweet into a list of words\n",
    "    \"\"\"\n",
    "    tokens = tknz.tokenize(s)\n",
    "    return [w for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ngrams(s, ngram):\n",
    "    \"\"\"\n",
    "    Tokenize each text into a list of words removing the ashtags in n-grams\n",
    "    \"\"\"\n",
    "    tokens =  ngrams(s, ngram)\n",
    "    return [w for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_frequency_df(series):\n",
    "    \"\"\"\n",
    "    Count each time the same word appeared in the series and returns a dataFrame\n",
    "    \"\"\"\n",
    "    corpus_lists = [doc for doc in series.dropna() if doc]\n",
    "    dictionary = Dictionary(corpus_lists)\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in corpus_lists]\n",
    "    token_freq_bow = defaultdict(int)\n",
    "    for token_id, token_sum in itertools.chain.from_iterable(corpus_bow):\n",
    "        token_freq_bow[token_id] += token_sum\n",
    "\n",
    "    return pd.DataFrame(list(token_freq_bow.items()), columns=['token_id', 'token_count']).assign(\n",
    "        token=lambda df1: df1.apply(lambda df2: dictionary.get(df2.token_id), axis=1),\n",
    "        doc_appeared=lambda df1: df1.apply(lambda df2: dictionary.dfs[df2.token_id], axis=1)).reindex(\n",
    "        labels=['token_id', 'token', 'token_count', 'doc_appeared'], axis=1).set_index('token_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_cloud(df, top_n):\n",
    "    \n",
    "    \"\"\"Creates a wordcloud based on term frequency of the first-n words\"\"\"\n",
    "    word_cloud = WordCloud(background_color='white', colormap='magma', contour_width=1,\n",
    "                           contour_color='orange', relative_scaling=0.5)\n",
    "\n",
    "    sorted_freq_dict = dict(df[['token', 'token_count']].nlargest(top_n, columns='token_count').values)\n",
    "    wc = word_cloud.generate_from_frequencies(frequencies=sorted_freq_dict, max_font_size=40)\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(15, 8))\n",
    "    ax.set_title('Term Frequency', fontsize=16)\n",
    "\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance(model , data, target):\n",
    "    \n",
    "    \"\"\" Creates a mini dataframe with all info on the model performance\"\"\"\n",
    "    \n",
    "    predictions = model.predict(data)\n",
    "    model_prob = model.predict_proba(data)[:,1]\n",
    "    \n",
    "    f1 = f1_score(target, predictions)\n",
    "    accuracy = accuracy_score(target, predictions)\n",
    "    roc_score = roc_auc_score(target, model_prob)\n",
    "    precision = precision_score(target, predictions)\n",
    "    \n",
    "    score = pd.DataFrame()\n",
    "    \n",
    "    score['f1'] = pd.Series(f1)\n",
    "    score['accuracy'] = pd.Series(accuracy)\n",
    "    score['roc_score'] = pd.Series(roc_score)\n",
    "    score['precision'] = pd.Series(precision)\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, train, validation, y_train, y_val):\n",
    "    \n",
    "    \"\"\"Plots the roc curves of two different sets\"\"\"\n",
    "    \n",
    "    base_pred_train = model.predict_proba(train)[:,1]\n",
    "    base_fpr_train, base_tpr_train, base_thresh_train = roc_curve(y_train, base_pred_train)\n",
    "\n",
    "    base_pred_validation = model.predict_proba(validation)[:,1]\n",
    "    base_fpr_validation, base_tpr_validation, base_thresh_validation = roc_curve(y_val, base_pred_validation)\n",
    "    \n",
    "    plt.style.use('seaborn')\n",
    "    plt.figure(figsize=(12,7))\n",
    "    ax1 = sns.lineplot(base_fpr_train, base_tpr_train, label='train',)\n",
    "    ax1.lines[0].set_color(\"orange\")\n",
    "    ax1.lines[0].set_linewidth(2)\n",
    "\n",
    "    ax2 = sns.lineplot(base_fpr_validation, base_tpr_validation, label='validaton')\n",
    "    ax2.lines[1].set_color(\"yellow\")\n",
    "    ax2.lines[1].set_linewidth(2)\n",
    "\n",
    "    ax3 = sns.lineplot([0,1], [0,1], label='baseline')\n",
    "    ax3.lines[2].set_linestyle(\"--\")\n",
    "    ax3.lines[2].set_color(\"black\")\n",
    "    ax3.lines[2].set_linewidth(2)\n",
    "\n",
    "    plt.title('Naive Bayes ROC Curve', fontsize=20)\n",
    "    plt.xlabel('FPR', fontsize=16)\n",
    "    plt.ylabel('TPR', fontsize=16)\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.text(x=0.8, y=0.8, s=\"50-50 guess\", fontsize=14,\n",
    "    bbox=dict(facecolor='whitesmoke', boxstyle=\"round, pad=0.4\"))\n",
    "\n",
    "    plt.legend(loc=4, fontsize=17)\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_seq_length(sequence):\n",
    "    \n",
    "    \"\"\"Returns the the maximum length of word sequences created by the \n",
    "    keras tokenizer\"\"\"\n",
    "    length = []\n",
    "    \n",
    "    for i in range(0, len(sequence)):\n",
    "        length.append(len(sequence[i]))\n",
    "    return max(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
